{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ede77d-b6c0-4a46-a3c7-4c4482bc43e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torchvision\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import backprop\n",
    "import snntorch.functional as SF\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54dfeccc-f515-4d59-ac96-e6093d3538da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim,\n",
    "                 layer_size,\n",
    "                 out_size,\n",
    "                 num_steps,\n",
    "                 alpha,\n",
    "                 beta,\n",
    "                 threshold,\n",
    "                 layer_means=[1.5, 1.5],\n",
    "                 layer_stds=[0.8, 0.8],\n",
    "                 bias=[0.0, 0.0],\n",
    "                 fc1_weight=None,\n",
    "                 fc2_weight=None,\n",
    "                 spike_grad=snn.surrogate.fast_sigmoid(slope=25)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        # initialize layers\n",
    "        self.fc1 = nn.Linear(in_dim, layer_size)\n",
    "        if fc1_weight is not None:\n",
    "            self.fc1.weight = torch.nn.Parameter(torch.Tensor(fc1_weight))\n",
    "        else:\n",
    "            nn.init.normal_(self.fc1.weight, layer_means[0], layer_stds[0])\n",
    "        nn.init.constant_(self.fc1.bias, bias[0])\n",
    "        \n",
    "        self.lif1 = snn.Synaptic(alpha=alpha, \n",
    "                                 beta=beta, \n",
    "                                 threshold=threshold, \n",
    "                                 reset_mechanism='subtract', \n",
    "                                 spike_grad=spike_grad)\n",
    "        \n",
    "        self.fc2 = nn.Linear(layer_size, out_size)\n",
    "        if fc2_weight is not None:\n",
    "            self.fc2.weight = torch.nn.Parameter(torch.Tensor(fc2_weight))\n",
    "        else:\n",
    "            nn.init.normal_(self.fc2.weight, layer_means[1], layer_stds[1])\n",
    "        nn.init.constant_(self.fc2.bias, bias[1])\n",
    "        \n",
    "        self.lif2 = snn.Synaptic(alpha=alpha, \n",
    "                                 beta=beta, \n",
    "                                 threshold=threshold, \n",
    "                                 reset_mechanism='subtract',\n",
    "                                 spike_grad=spike_grad)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        syn1, mem1 = self.lif1.init_synaptic()\n",
    "        syn2, mem2 = self.lif2.init_synaptic()\n",
    "        \n",
    "        spk1_rec=[]\n",
    "        mem1_rec=[]\n",
    "\n",
    "        spk2_rec = []  # Record the output trace of spikes\n",
    "        mem2_rec = []  # Record the output trace of membrane potential\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "\n",
    "            cur1 = self.fc1(x[step])\n",
    "            spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1)\n",
    "            \n",
    "            spk1_rec.append(spk1)\n",
    "            mem1_rec.append(mem1)\n",
    "            \n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
    "\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return [torch.stack(spk1_rec, dim=0), \n",
    "                torch.stack(mem1_rec, dim=0),\n",
    "                torch.stack(spk2_rec, dim=0), \n",
    "                torch.stack(mem2_rec, dim=0)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27fcc754-1fd4-449e-980f-fa114d5f5611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.data.dataset import Dataset\n",
    "        \n",
    "class YinYangDataset(Dataset):\n",
    "    def __init__(self, r_small=0.1, r_big=0.5, size=1000, seed=42, transform=None, **kwargs):\n",
    "        super(YinYangDataset, self).__init__()\n",
    "        # using a numpy RNG to allow compatibility to other deep learning frameworks\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.transform = transform\n",
    "        self.r_small = r_small\n",
    "        self.r_big = r_big\n",
    "        self.__vals = []\n",
    "        self.__cs = []\n",
    "        self.class_names = ['yin', 'yang', 'dot']\n",
    "        for i in range(size):\n",
    "            # keep num of class instances balanced by using rejection sampling\n",
    "            # choose class for this sample\n",
    "            goal_class = self.rng.randint(3)\n",
    "            x, y, c = self.get_sample(goal=goal_class)\n",
    "            # add mirrod axis values\n",
    "            x_flipped = 1. - x\n",
    "            y_flipped = 1. - y\n",
    "            val = np.array([x, y, x_flipped, y_flipped])\n",
    "            self.__vals.append(val)\n",
    "            self.__cs.append(c)\n",
    "\n",
    "    def get_sample(self, goal=None):\n",
    "        # sample until goal is satisfied\n",
    "        found_sample_yet = False\n",
    "        while not found_sample_yet:\n",
    "            # sample x,y coordinates\n",
    "            x, y = self.rng.rand(2) * 2. * self.r_big\n",
    "            # check if within yin-yang circle\n",
    "            if np.sqrt((x - self.r_big)**2 + (y - self.r_big)**2) > self.r_big:\n",
    "                continue\n",
    "            # check if they have the same class as the goal for this sample\n",
    "            c = self.which_class(x, y)\n",
    "            if goal is None or c == goal:\n",
    "                found_sample_yet = True\n",
    "                break\n",
    "        return x, y, c\n",
    "\n",
    "    def which_class(self, x, y):\n",
    "        # equations inspired by\n",
    "        # https://link.springer.com/content/pdf/10.1007/11564126_19.pdf\n",
    "        d_right = self.dist_to_right_dot(x, y)\n",
    "        d_left = self.dist_to_left_dot(x, y)\n",
    "        criterion1 = d_right <= self.r_small\n",
    "        criterion2 = d_left > self.r_small and d_left <= 0.5 * self.r_big\n",
    "        criterion3 = y > self.r_big and d_right > 0.5 * self.r_big\n",
    "        is_yin = criterion1 or criterion2 or criterion3\n",
    "        is_circles = d_right < self.r_small or d_left < self.r_small\n",
    "        if is_circles:\n",
    "            return 2\n",
    "        return int(is_yin)\n",
    "\n",
    "    def dist_to_right_dot(self, x, y):\n",
    "        return np.sqrt((x - 1.5 * self.r_big)**2 + (y - self.r_big)**2)\n",
    "\n",
    "    def dist_to_left_dot(self, x, y):\n",
    "        return np.sqrt((x - 0.5 * self.r_big)**2 + (y - self.r_big)**2)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = (self.__vals[index].copy(), self.__cs[index])\n",
    "        if self.transform:\n",
    "            sample = (self.transform(sample[0]), sample[1])\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__cs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81bcb753-6d87-4b68-b122-bf7be965f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_single_input(x, num_steps, min_t, max_t, bias_t):\n",
    "    x = x * (max_t-min_t) + min_t\n",
    "    idx = x.searchsorted(bias_t)\n",
    "    x = np.insert(x, idx, bias_t)\n",
    "    x = torch.Tensor(x)\n",
    "    x = torch.round((num_steps/max_t)*x)\n",
    "    x = F.one_hot(x.to(torch.int64), num_steps + 1)\n",
    "    x = x.to(torch.float32)\n",
    "    return x.squeeze().T\n",
    "\n",
    "\n",
    "class SpikeTimeTransform(object):\n",
    "    \"\"\"Transform inputs into spike times, given min, max, and bias times\n",
    "\n",
    "    Args:\n",
    "        num_steps (int): Number of time steps; i.e., (max_t - min_t)/num_steps is dt\n",
    "        min_t (int): Minimum time\n",
    "        max_t (int): Maximum time\n",
    "        orig_max (int): The maximum value of the original data\n",
    "        orig_min (int): The minimum value of the original data\n",
    "    \"\"\"\n",
    "    def __init__(self, num_steps, min_t, max_t, bias_t):\n",
    "        self.num_steps = num_steps\n",
    "        self.min_t = min_t\n",
    "        self.max_t = max_t\n",
    "        self.bias_t = bias_t\n",
    "        \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return transform_single_input(x,\n",
    "                                      self.num_steps, \n",
    "                                      self.min_t,\n",
    "                                      self.max_t,\n",
    "                                      self.bias_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "2bd78486-57f2-4f75-b329-70c4f8def5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 2000\n",
    "min_t = 0.15\n",
    "max_t = 2.0\n",
    "bias_t = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "4d0c340a-7164-496b-a9b2-6ae5887d68e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = YinYangDataset(size=5000, \n",
    "                               seed=42, \n",
    "                               transform=SpikeTimeTransform(num_steps, min_t, max_t, bias_t)\n",
    "                              )\n",
    "dataset_test = YinYangDataset(size=1000, \n",
    "                              seed=40,\n",
    "                              transform=SpikeTimeTransform(num_steps, min_t, max_t, bias_t)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "0843deb2-aeb5-46d1-b42c-1eafd593dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize_train = 150\n",
    "batchsize_eval = len(dataset_test)\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batchsize_train, shuffle=False)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batchsize_eval, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "00304fc1-56d8-4b84-833b-da4564e3a486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150, 2001, 5])\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_loader:\n",
    "    print(data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "3327800e-3ff1-460e-8683-ff04485bcd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "num_inputs = 5\n",
    "num_hidden = 150\n",
    "num_classes = 3\n",
    "\n",
    "# alpha, beta, theta need to rescale for loss vs. network\n",
    "BETA = 0.5\n",
    "ALPHA = 1.0\n",
    "THETA = 1.0\n",
    "\n",
    "dt = max_t/num_steps\n",
    "\n",
    "# network params\n",
    "ALPHA_S = np.exp(-dt * ALPHA)\n",
    "BETA_S = np.exp(-dt * BETA)\n",
    "THETA_S = THETA * (num_steps/max_t)\n",
    "\n",
    "# loss hyper params\n",
    "TAU_0 = 1.0 * dt\n",
    "TAU_1 = 1.0\n",
    "GAMMA = 0\n",
    "\n",
    "# optimizer params\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.999\n",
    "EPS = 1e-8\n",
    "ETA = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "310f905b-b71b-4034-b011-02b4a82f614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_weight = np.array([np.random.normal(1.5, 0.8, num_inputs) for i in range(num_hidden)])\n",
    "fc2_weight = np.array([np.random.normal(2.0, 0.1, num_hidden) for i in range(num_classes)])\n",
    "\n",
    "with open('weights.npy', 'wb') as f:\n",
    "    np.save(f, fc1_weight)\n",
    "    np.save(f, fc2_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "24b38e18-af7c-4c64-93e9-569e5a3c5fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('weights.npy', 'rb') as f:\n",
    "    fc1_weight = np.load(f)\n",
    "    fc2_weight = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "675de8fc-bda1-4e4e-8874-599547b98209",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(\n",
    "    num_inputs,\n",
    "    num_hidden,\n",
    "    num_classes, \n",
    "    num_steps, \n",
    "    ALPHA_S, \n",
    "    BETA_S, \n",
    "    THETA_S,\n",
    "    # layer_means=[1.0, 1.0],\n",
    "    # layer_stds=[0.8, 0.8],\n",
    "    fc1_weight=fc1_weight,\n",
    "    fc2_weight=fc2_weight,\n",
    "    spike_grad=snn.surrogate.fast_sigmoid(slope=10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "dffd3ae1-9b10-49bf-89da-e22495643b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    net.parameters(), \n",
    "    lr=ETA,\n",
    "    betas=(BETA_1, BETA_2),\n",
    "    eps=EPS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "3e6ae3fb-932e-4644-a6e1-4dc26dbf6f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = SF.ce_temporal_loss()\n",
    "loss_fn = SF.ce_rate_loss()\n",
    "# loss_fn = SF.ce_count_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "c6b9f42e-676c-4884-9ea6-fda67cb81f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# set training parameters\n",
    "n_epochs = 5\n",
    "train_accuracies = []\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "939e542e-1855-4477-a474-785c4dc8db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_batch(bsz, data, labels, model, loss):\n",
    "    spk1, mem1, spk2, mem2 = model(torch.swapaxes(data, 0, 1))\n",
    "    \n",
    "    # acc = SF.accuracy_temporal(spk2, labels.squeeze())\n",
    "    acc = SF.accuracy_rate(spk2, labels.squeeze())\n",
    "    # l = loss(torch.mean(spk2, dim=0, keepdim=True), labels.squeeze())\n",
    "    l = loss(spk2, labels.squeeze())\n",
    "    \n",
    "    return acc, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "5a159591-da3e-4f8b-8d70-99a12277f65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed (sec)= 22.472201267000855\n",
      "epoch= 0\n",
      "loss= tensor([1.1342], grad_fn=<DivBackward0>)\n",
      "acc= 0.30666666666666664\n",
      "Time elapsed (sec)= 74.31982344400058\n",
      "epoch= 0\n",
      "loss= tensor([1.1362], grad_fn=<DivBackward0>)\n",
      "acc= 0.36\n",
      "Time elapsed (sec)= 132.3275370070005\n",
      "epoch= 0\n",
      "loss= tensor([1.1275], grad_fn=<DivBackward0>)\n",
      "acc= 0.38666666666666666\n",
      "Time elapsed (sec)= 187.02013804300077\n",
      "epoch= 0\n",
      "loss= tensor([1.1229], grad_fn=<DivBackward0>)\n",
      "acc= 0.3\n",
      "Time elapsed (sec)= 215.79404171700116\n",
      "epoch= 1\n",
      "loss= tensor([1.1221], grad_fn=<DivBackward0>)\n",
      "acc= 0.30666666666666664\n",
      "Time elapsed (sec)= 267.2445158700011\n",
      "epoch= 1\n",
      "loss= tensor([1.1203], grad_fn=<DivBackward0>)\n",
      "acc= 0.36\n",
      "Time elapsed (sec)= 319.58716847600044\n",
      "epoch= 1\n",
      "loss= tensor([1.1185], grad_fn=<DivBackward0>)\n",
      "acc= 0.38666666666666666\n",
      "Time elapsed (sec)= 371.0066950320015\n",
      "epoch= 1\n",
      "loss= tensor([1.1185], grad_fn=<DivBackward0>)\n",
      "acc= 0.3\n",
      "Time elapsed (sec)= 396.8284662160004\n",
      "epoch= 2\n",
      "loss= tensor([1.1182], grad_fn=<DivBackward0>)\n",
      "acc= 0.30666666666666664\n",
      "Time elapsed (sec)= 461.0587524170005\n",
      "epoch= 2\n",
      "loss= tensor([1.1179], grad_fn=<DivBackward0>)\n",
      "acc= 0.36\n",
      "Time elapsed (sec)= 518.4238736930001\n",
      "epoch= 2\n",
      "loss= tensor([1.1173], grad_fn=<DivBackward0>)\n",
      "acc= 0.38666666666666666\n",
      "Time elapsed (sec)= 587.9548874709999\n",
      "epoch= 2\n",
      "loss= tensor([1.1183], grad_fn=<DivBackward0>)\n",
      "acc= 0.3\n",
      "Time elapsed (sec)= 604.5180388560002\n",
      "epoch= 3\n",
      "loss= tensor([1.1181], grad_fn=<DivBackward0>)\n",
      "acc= 0.30666666666666664\n",
      "Time elapsed (sec)= 673.9024363290009\n",
      "epoch= 3\n",
      "loss= tensor([1.1179], grad_fn=<DivBackward0>)\n",
      "acc= 0.36\n",
      "Time elapsed (sec)= 745.054144873\n",
      "epoch= 3\n",
      "loss= tensor([1.1165], grad_fn=<DivBackward0>)\n",
      "acc= 0.38666666666666666\n",
      "Time elapsed (sec)= 826.65790724\n",
      "epoch= 3\n",
      "loss= tensor([1.1167], grad_fn=<DivBackward0>)\n",
      "acc= 0.3\n",
      "Time elapsed (sec)= 840.5239213730001\n",
      "epoch= 4\n",
      "loss= tensor([1.1167], grad_fn=<DivBackward0>)\n",
      "acc= 0.30666666666666664\n",
      "Time elapsed (sec)= 924.7104837440002\n",
      "epoch= 4\n",
      "loss= tensor([1.1164], grad_fn=<DivBackward0>)\n",
      "acc= 0.36\n",
      "Time elapsed (sec)= 1010.6252895340003\n",
      "epoch= 4\n",
      "loss= tensor([1.1157], grad_fn=<DivBackward0>)\n",
      "acc= 0.38666666666666666\n",
      "Time elapsed (sec)= 1054.035917741001\n",
      "epoch= 4\n",
      "loss= tensor([1.1161], grad_fn=<DivBackward0>)\n",
      "acc= 0.3\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "for k in range(n_epochs):\n",
    "    for i, (batch, batch_labels) in enumerate(train_loader):\n",
    "\n",
    "        bsz = len(batch)\n",
    "        \n",
    "        batch_acc, batch_loss = forward_batch(bsz, batch, batch_labels, net, loss_fn)\n",
    "\n",
    "        train_accuracies.append(batch_acc)\n",
    "        train_losses.append(batch_loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_loss.backward()\n",
    "\n",
    "        optimizer.step()  # take optimizer step\n",
    "\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            end_time = time.perf_counter()\n",
    "            print(\"Time elapsed (sec)=\", end_time - start_time)\n",
    "            print(\"epoch=\", k)\n",
    "            print(\"loss=\", batch_loss)\n",
    "            print(\"acc=\", batch_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "a41dcc70-14aa-431f-9348-3f41530d2132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward_batch(bsz, batch, batch_labels, net, loss_fn)\n",
    "spk1, mem1, spk2, mem2 = net(torch.swapaxes(batch, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "43a09c8f-b4ce-40ff-a1a5-c6e9bb013fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 694,  696,  700,  ..., 1999, 1999, 1999]),\n",
       " tensor([1, 0, 2,  ..., 0, 1, 2]))"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(spk2[:, 0, :], as_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1b2ef89b-4d7a-4c87-b66c-6c0989900e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_loss.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "91221c55-8fd1-4a85-8437-fba13ff117b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_acc = []\n",
    "eval_losses = []\n",
    "\n",
    "for i, (batch, batch_labels) in enumerate(test_loader):\n",
    "    bsz = len(batch)\n",
    "\n",
    "    batch_acc, batch_loss = forward_batch(bsz, batch, batch_labels, net, loss_fn)\n",
    "    \n",
    "    eval_acc.append(batch_acc)\n",
    "    eval_losses.append(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "f7dcd27e-3188-4571-a128-17e56a183118",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = torch.stack(train_losses, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494182c-867a-437b-ab60-fbfac7993572",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_losses = torch.stack(eval_losses, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "a59ae6d2-eb7c-44ee-b8ce-1fc9868c5409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ce294880>]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses.squeeze().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "e790363e-c346-4d19-b812-bca76be0bc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proj False\n"
     ]
    }
   ],
   "source": [
    "import sortednp\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg\n",
    "from scipy.special import log_softmax\n",
    "from scipy.special import softmax\n",
    "from numpy_ml.neural_nets.optimizers import Adam\n",
    "\n",
    "# Define neuron behavior for SNN\n",
    "\n",
    "proj = False\n",
    "\n",
    "print('proj', proj)\n",
    "\n",
    "class LIFNeuron(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,\n",
    "        beta,  # inverse membrane time constant\n",
    "        alpha=0.0,  # inverse synaptic time constant\n",
    "        threshold=1.0,\n",
    "        weight=None,\n",
    "        layer_idx=0,\n",
    "        neuron_idx=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.beta = beta\n",
    "        self.alpha = alpha\n",
    "        self.threshold = threshold\n",
    "        if weight is None:\n",
    "            self.weight = np.random.randn(self.in_dim)  # change later, other initializers\n",
    "        \n",
    "        self.layer_idx = layer_idx\n",
    "        self.neuron_idx = neuron_idx\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs, T, weight=None):\n",
    "        \"\"\"\n",
    "        x - input firing times \n",
    "        T - maximum time horizon\n",
    "        \"\"\"\n",
    "        if weight is None:\n",
    "            w = self.weight\n",
    "        else:\n",
    "            w = weight\n",
    "            \n",
    "        if type(inputs) is np.ndarray and len(inputs.shape) < 2:\n",
    "            inputs = np.expand_dims(inputs, axis=0)\n",
    "            \n",
    "        if type(w) is not list and type(w) is not np.ndarray:\n",
    "            w = [w]\n",
    "        \n",
    "        return self._evolve(\n",
    "            T,\n",
    "            inputs,\n",
    "            w,\n",
    "            alpha=self.alpha,\n",
    "            beta=self.beta,\n",
    "            theta=self.threshold\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # function for estimating the firing time\n",
    "    def _evolve(self, t_max, spikes, w, A=0, B=0, alpha=2, beta=1, theta=1):\n",
    "        \n",
    "        # check the validity of alpha and beta\n",
    "        if alpha==beta:\n",
    "            raise ValueError('There might be some numerical instability for equal alpha and beta.')\n",
    "            \n",
    "        # check that inputs are sorted... or sort them\n",
    "        spikes = [spikes[i] if np.all(spikes[i][:-1] <= spikes[i][1:]) else np.sort(spikes[i]) \\\n",
    "                  for i in range(len(spikes))]\n",
    "\n",
    "        # number of neurons\n",
    "        N=len(spikes)\n",
    "\n",
    "        # produce the corresponding weights\n",
    "        w_mat=[]\n",
    "        idx=[]\n",
    "        for i, el in enumerate(w):\n",
    "            leni=len(spikes[i])\n",
    "            w_mat.append(np.array([w[i]]*leni))\n",
    "            idx.append(np.array([i]*leni))\n",
    "        \n",
    "        spk=None\n",
    "        wt=None\n",
    "        neuron=None\n",
    "        for i in range(N):\n",
    "            if spikes[i].size > 0:\n",
    "                if spk is None:\n",
    "                    spk=spikes[i]\n",
    "                    wt=w_mat[i]\n",
    "                    neuron=idx[i]\n",
    "                else:\n",
    "                    spk=np.concatenate((spk, spikes[i]))\n",
    "                    wt=np.concatenate((wt, w_mat[i]))\n",
    "                    neuron=np.concatenate((neuron, idx[i]))\n",
    "                    \n",
    "        # inputs are all empty...\n",
    "        if spk is None:\n",
    "            return np.array([]), w, {}, {}\n",
    "\n",
    "        # sort the firing time and keep the same order for weights\n",
    "        order=np.argsort(spk)\n",
    "    \n",
    "        spk=spk[order]\n",
    "        wt=wt[order]\n",
    "        neuron=neuron[order]\n",
    "\n",
    "        # reference time and next firing time\n",
    "        start=0\n",
    "\n",
    "        fire_max=t_max\n",
    "        fire=fire_max\n",
    "\n",
    "        # list to save firing time\n",
    "        fire_time=[]\n",
    "\n",
    "        # define the iterator\n",
    "        spk_iter=iter(zip(spk,wt,neuron))\n",
    "\n",
    "        # Create causal edges\n",
    "        M_f = {}\n",
    "        queue = []\n",
    "        \n",
    "        while (fwn:=next(spk_iter,None)) is not None:\n",
    "            # read the weight and firing time of the next spike\n",
    "            f,w,n=fwn\n",
    "\n",
    "            # check to see if the firing time \"f\" is larger than the next predicted \n",
    "            # firing time\n",
    "            if f>fire:\n",
    "                # fire is valid firing time because it happens before \"f\"\n",
    "\n",
    "                # (1) save this firing time as valid one\n",
    "                fire_time.append(fire)\n",
    "                \n",
    "                # Update causal diagram\n",
    "                M_f[fire] = queue.copy()\n",
    "\n",
    "                # (2) update the potentials due to firing\n",
    "                A=A*np.exp(-alpha*(fire-start))\n",
    "                B=B*np.exp(-beta*(fire-start))-theta # the effect of membrane reset\n",
    "\n",
    "                # (3) change the start to the latest firing time\n",
    "                start=fire\n",
    "\n",
    "                #print(\"potential at the firing time=\", A, B, A+B)\n",
    "\n",
    "                # (4) tricky part:\n",
    "                # it may happen that even after potential reset, the neuron has several\n",
    "                # new firings even before it receives a new spike from its neigboring neurons\n",
    "                # we should add all these firing time\n",
    "\n",
    "                while True:\n",
    "                    # a simple sufficient condition\n",
    "                    # if the derivative is negative no new firing can happen\n",
    "                    if -(A*alpha+B*beta)<0:\n",
    "                        fire=fire_max+1\n",
    "                        break\n",
    "\n",
    "                    # otherwise find the next root starting from the reference\n",
    "                    fire_rel=self._solve(A,alpha,B,beta,theta)\n",
    "\n",
    "                    # take into account the effect of reference point\n",
    "\n",
    "                    if fire_rel==None or fire_rel+start>f:\n",
    "                        fire=fire_max+1\n",
    "                        break\n",
    "\n",
    "                    # otherwise the firing time is a valid one\n",
    "                    # (1) save the firing time\n",
    "                    # take the reference into account\n",
    "                    fire=fire_rel+start\n",
    "\n",
    "                    fire_time.append(fire)\n",
    "                    M_f[fire] = queue.copy()\n",
    "\n",
    "                    # (2) update the potentials due to firing\n",
    "                    A=A*np.exp(-alpha*(fire_rel))\n",
    "                    B=B*np.exp(-beta*(fire_rel))-theta # the effect of membrane reset\n",
    "\n",
    "                    #print(\"potential at the firing time=\", A, B, A+B)\n",
    "\n",
    "                    # (3) change the start to the latest firing time\n",
    "                    start=fire\n",
    "\n",
    "                    # (4) continue this procedure to register all in-between firing times\n",
    "                \n",
    "                # empty the queue\n",
    "                # queue = []\n",
    "                    \n",
    "            queue.append((f,n))\n",
    "\n",
    "            # now there are no residula firing times from past\n",
    "            # the new input spike should be processed\n",
    "\n",
    "            # update A and B until the current firing time \"f\"\n",
    "            A=A*np.exp(-alpha*(f-start))\n",
    "            B=B*np.exp(-beta*(f-start))\n",
    "\n",
    "            # add the effect of new spike\n",
    "            A+=w/(beta-alpha)\n",
    "            B+=w/(alpha-beta)\n",
    "\n",
    "\n",
    "            # also adjust the reference time\n",
    "            start=f\n",
    "\n",
    "            # find the firing time for the updated kernel\n",
    "            fire_rel=self._solve(A, alpha, B, beta, theta)\n",
    "\n",
    "            # if there is no firing time: set fire to infinity (here fire_max+1)\n",
    "            if fire_rel==None:\n",
    "                fire=fire_max+1\n",
    "                #print(\"No firing time in this interval\")\n",
    "            else:\n",
    "                # adjust the firing time with respect to the reference point\n",
    "                fire=fire_rel+start\n",
    "                #print(\"speculated firing time: \", fire)\n",
    "\n",
    "        # at this stage all input spikes are processed but the neuron\n",
    "        # may keep producing spikes due to its residual potential\n",
    "        # we need to add also those firing times\n",
    "\n",
    "        while fire<fire_max:\n",
    "            # still some residual firing left\n",
    "\n",
    "            # add fire to the list\n",
    "            fire_time.append(fire)\n",
    "            M_f[fire] = queue.copy()\n",
    "            \n",
    "\n",
    "            # adjust the parameters\n",
    "            A=A*np.exp(-alpha*(fire-start))\n",
    "            B=B*np.exp(-beta*(fire-start))-theta\n",
    "\n",
    "            # adjust the reference time\n",
    "            start=fire\n",
    "\n",
    "            # a simple sufficient condition\n",
    "            # if the derivative is negative no new firing can happen\n",
    "            if -(A*alpha+B*beta)<0:\n",
    "                fire=fire_max+1\n",
    "                break\n",
    "\n",
    "            # otherwise find the next root starting from the reference\n",
    "            fire_rel=self._solve(A,alpha,B,beta,theta)\n",
    "\n",
    "            # take into account the effect of reference point and decide\n",
    "            if fire_rel==None or fire_rel+start>fire_max:\n",
    "                fire=fire_max+1\n",
    "                break\n",
    "            else:\n",
    "                # adjust the firing time with respect to the refernce\n",
    "                # repeat the loop\n",
    "                fire=fire_rel+start\n",
    "                \n",
    "        # return the produced spikes (and other info)\n",
    "        fire_time=np.asarray(fire_time)\n",
    "        M_n={}  # layer_idx/neuron_idx - list<firing time, neuron>\n",
    "        \n",
    "        if fire_time.size < 1:\n",
    "            M_n[(self.layer_idx, self.neuron_idx)]=np.array([])    \n",
    "        else:\n",
    "            M_n[(self.layer_idx, self.neuron_idx)] = fire_time\n",
    "        \n",
    "        return fire_time, w, M_f, M_n\n",
    "    \n",
    "    \n",
    "    def _solve(self, A, alpha, B, beta, theta, rel_err=0.0000001):\n",
    "        \"\"\"\n",
    "        A, alpha: weights corresponding to the synapse\n",
    "        B, beta: weights corresponding to the membrane\n",
    "        theta: the firing threshold\n",
    "\n",
    "        aim: solve\n",
    "            A exp(-alpha t) + B exp(-beta t) = theta\n",
    "        \"\"\"\n",
    "\n",
    "        #============= find the value and the derivative of function at t=0 =======\n",
    "        # value of the function at t=0\n",
    "        val_0= A+B\n",
    "\n",
    "        # derivative of the function at t=0\n",
    "        der_0=-alpha*A-beta*B\n",
    "\n",
    "\n",
    "        #=========== find the extremum point in [0, infty]==========\n",
    "    \n",
    "        #----------- compute the extremum point parameter --------\n",
    "        # if this parameter is negative the function is monotone\n",
    "        par_ext=-(beta*B)/(alpha*A)\n",
    "\n",
    "        # compute the extremeum point \n",
    "        if par_ext<0:\n",
    "            t_ext=0\n",
    "        else:\n",
    "            if alpha!=beta:\n",
    "                t_ext= np.maximum(np.log(par_ext)/(beta-alpha), 0.)\n",
    "            else:\n",
    "                # use l'Hopital rule\n",
    "                t_ext=np.log(-B/A)*(1/alpha)\n",
    "\n",
    "        # the extremum value\n",
    "        val_ext=A*np.exp(-alpha*t_ext) + B*np.exp(-beta*t_ext)\n",
    "\n",
    "\n",
    "        #================ decide based on the info ========\n",
    "        if val_0>theta and val_ext>theta:\n",
    "            # the function is increasing at first and starts above theta\n",
    "            t_min=t_ext\n",
    "            t_max=np.inf\n",
    "        elif val_0>theta and val_ext<theta:\n",
    "            # the function has a minimum\n",
    "            t_min=0\n",
    "            t_max=t_ext\n",
    "        elif val_0<theta and val_ext>theta:\n",
    "            # the function is increasing at first and has a maximum\n",
    "            t_min=0\n",
    "            t_max=t_ext\n",
    "        elif val_0<theta and val_ext<theta:\n",
    "            # the function starts below theta and is decreasing at first\n",
    "            # it cannot intersect theta\n",
    "            return None\n",
    "        else:\n",
    "            print(\"val_0=\", val_0)\n",
    "            print(\"val_ext=\", val_ext)\n",
    "            return None\n",
    "\n",
    "        #================= refine the search interval ==================\n",
    "\n",
    "        #------------ first: solve the problem of t_max in case it is None -------\n",
    "        if t_max==np.inf:\n",
    "            ab=alpha if alpha<beta else beta\n",
    "            t_max=1/ab\n",
    "            scale=2\n",
    "            while True:\n",
    "                # evaluate the function\n",
    "                val_max=A*np.exp(-alpha*t_max) + B*np.exp(-beta*t_max)\n",
    "                if val_max < theta:\n",
    "                    break\n",
    "\n",
    "                # if not yet below the threshold theta: scale t_max\n",
    "                t_max=scale*t_max\n",
    "\n",
    "        #------------ Now t_min and t_max are known:=> refine the interval --------\n",
    "        val_min=A*np.exp(-alpha*t_min)+B*np.exp(-beta*t_min)\n",
    "        val_max=A*np.exp(-alpha*t_max)+B*np.exp(-beta*t_max)\n",
    "\n",
    "        while True:\n",
    "            # find the point in the middle\n",
    "            t_mid=(t_min + t_max)/2\n",
    "            val_mid=A*np.exp(-alpha*t_mid)+B*np.exp(-beta*t_mid)\n",
    "\n",
    "            # check the condition\n",
    "            if (val_mid>theta and val_max>=theta) or (val_mid<theta and val_max<theta):\n",
    "                t_max=t_mid\n",
    "            else:\n",
    "                t_min=t_mid\n",
    "\n",
    "            # compute the precision\n",
    "            dt=t_max-t_min\n",
    "            t_avg=(t_max+t_min)/2\n",
    "            if dt/t_avg < rel_err:\n",
    "                break\n",
    "\n",
    "        return t_avg\n",
    "    \n",
    "\n",
    "    \n",
    "class FeedForwardSNN(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,\n",
    "        beta,  # inverse membrane time constant\n",
    "        alpha=0.0,  # inverse synaptic time constant\n",
    "        threshold=1.0,\n",
    "        layer_sizes=None,\n",
    "        weights=None\n",
    "    ):  \n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.beta = beta\n",
    "        self.alpha = alpha\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        if not layer_sizes:\n",
    "            self.layer_sizes = [input_size]\n",
    "        else:\n",
    "            self.layer_sizes = layer_sizes\n",
    "        \n",
    "        self.layer = [[] for sz in range(len(self.layer_sizes))]\n",
    "        self.weights = weights  # change this later, proper initializer\n",
    "        \n",
    "        \n",
    "    def build(self):  # don't need this explicitly if using torch\n",
    "        for i, sz in enumerate(self.layer_sizes):\n",
    "            for j in range(sz):\n",
    "                self.layer[i].append(\n",
    "                    LIFNeuron(\n",
    "                        self.in_dim if i == 0 else self.layer_sizes[i-1], \n",
    "                        self.beta, \n",
    "                        alpha=self.alpha,\n",
    "                        threshold=self.threshold,\n",
    "                        weight=None if self.weights is None else self.weights[i][j],\n",
    "                        layer_idx=i,\n",
    "                        neuron_idx=j\n",
    "                    )\n",
    "                )        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def forward(self, inputs, T, weights=None):\n",
    "        if not weights:\n",
    "            assert self.weights is not None\n",
    "            weights = self.weights\n",
    "            \n",
    "        ret=[[] for i in range(len(self.weights))]\n",
    "        \n",
    "        r_o = [np.zeros(len(w)) for w in self.weights]\n",
    "        \n",
    "        M_f={}\n",
    "        M_n={}\n",
    "        \n",
    "        if type(inputs) is list:\n",
    "            inputs = np.asarray(inputs)\n",
    "        \n",
    "        prev_out=inputs\n",
    "        \n",
    "        for i, layer in enumerate(self.layer):\n",
    "            curr_out = []\n",
    "            for j, neuron in enumerate(layer):\n",
    "                out, w, out_f, out_n = neuron.forward(inputs=prev_out, T=T, weight=weights[i][j])\n",
    "                r_o[i][j] = np.size(out)/T\n",
    "                M_f={**M_f, **out_f}\n",
    "                M_n={**M_n, **out_n}\n",
    "                curr_out.append(out)\n",
    "            ret[i] = curr_out\n",
    "            prev_out = curr_out\n",
    "\n",
    "        return inputs, ret, weights, M_f, M_n, r_o\n",
    "    \n",
    "    \n",
    "    def update(self, new_weights, T, r_os=None, zeta_0=-np.inf):\n",
    "        \n",
    "        # Project weights onto affine space to keep SNN firing\n",
    "        if proj:\n",
    "            for r_o in r_os:\n",
    "                _, idxs = self._calculate_zeta(r_o, new_weights, T, zeta_0)\n",
    "                new_weights = self._project_w(r_o, new_weights, T, idxs, zeta_0)\n",
    "            \n",
    "        self.weights = new_weights\n",
    "        \n",
    "    \n",
    "    def _calculate_zeta(self, r_o, weights, T, zeta_0=-np.inf):\n",
    "        alpha=self.alpha\n",
    "        beta=self.beta\n",
    "        theta=self.threshold\n",
    "        in_dim=self.in_dim\n",
    "\n",
    "        zeta = [np.zeros_like(r) for r in r_o]\n",
    "        idxs = []\n",
    "\n",
    "        for layer in range(len(weights)):\n",
    "            for neuron, w in enumerate(weights[layer]):\n",
    "                r_i = np.ones_like(w)/T if layer == 0 else r_o[layer - 1]\n",
    "                wra = np.dot(w, r_i)/alpha\n",
    "                rth = r_o[layer][neuron] * theta\n",
    "                z = (wra - rth)/(beta * theta)\n",
    "                if z < zeta_0:\n",
    "                    idxs.append((layer, neuron))\n",
    "                zeta[layer][neuron] = z\n",
    "        return zeta, idxs\n",
    "    \n",
    "    \n",
    "    def _project_w(self, r_o, weights, T, idx, zeta_0):\n",
    "        alpha=self.alpha\n",
    "        beta=self.beta\n",
    "        theta=self.threshold\n",
    "        in_dim=self.in_dim\n",
    "\n",
    "        proj = [np.copy(w) for w in weights]\n",
    "        for layer in range(len(weights)):\n",
    "            for neuron, w in enumerate(weights[layer]):\n",
    "                if (layer, neuron) not in idx:\n",
    "                    continue\n",
    "                else:\n",
    "                    ra = np.ones_like(w)/T if layer == 0 else r_o[layer - 1]\n",
    "                    ra = ra/alpha\n",
    "                    wra = np.dot(w, ra)\n",
    "                    th = theta * (r_o[layer][neuron] + zeta_0 * beta)\n",
    "                    N = np.dot(ra, ra)\n",
    "\n",
    "                    num = (wra - th)/N\n",
    "                    proj[layer][neuron] = w - np.multiply(num, ra)\n",
    "        return proj\n",
    "        \n",
    "        \n",
    "        \n",
    "class SNNUtil:\n",
    "    \n",
    "    @staticmethod\n",
    "    def V_w(fs, prev, alpha, beta, theta):\n",
    "        v_w=(np.exp(-alpha*(fs-prev))-np.exp(-beta*(fs-prev)))/(beta-alpha)*np.heaviside(fs-prev,0)\n",
    "        return v_w\n",
    "\n",
    "    @staticmethod\n",
    "    def V_fs(fs, prev, w, alpha, beta, theta):\n",
    "        v_fs=w*(-alpha*np.exp(-alpha*(fs-prev))+beta*np.exp(-beta*(fs-prev)))/(beta-alpha)*np.heaviside(fs-prev,0)\n",
    "        return v_fs\n",
    "\n",
    "    @staticmethod\n",
    "    def V_fp(fs, prev, w, alpha, beta, theta):\n",
    "        v_fp=w*(alpha*np.exp(-alpha*(fs-prev))-beta*np.exp(-beta*(fs-prev)))/(beta-alpha)*np.heaviside(fs-prev,0)\n",
    "        return v_fp\n",
    "    \n",
    "\n",
    "    \n",
    "class SnnCrossEntropy(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        T,\n",
    "        alpha,\n",
    "        beta,\n",
    "        theta,\n",
    "        tau_0,\n",
    "        tau_1,\n",
    "        gamma,\n",
    "    ):\n",
    "        self.num_classes = num_classes\n",
    "        self.T = T\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.theta = theta\n",
    "        self.tau_0 = tau_0\n",
    "        self.tau_1 = tau_1\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        \n",
    "    def _first_spikes(self, outputs):\n",
    "        assert len(outputs) == self.num_classes\n",
    "        return np.asarray([np.min(outputs[i], initial=self.T) for i in range(len(outputs))])\n",
    "    \n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        outputs,\n",
    "        label, \n",
    "    ):\n",
    "        # outputs is a list of np.array containing firing times\n",
    "        # label is a number in [K] where K is number of classes\n",
    "        \n",
    "        first_spks = self._first_spikes(outputs)\n",
    "        reg_term = np.exp(first_spks[label] / self.tau_1) - 1\n",
    "        return -(log_softmax(-first_spks / self.tau_0)[label] + self.gamma * reg_term)\n",
    "\n",
    "    \n",
    "    def backward(\n",
    "        self,\n",
    "        inputs,\n",
    "        outputs,\n",
    "        weights,\n",
    "        label,\n",
    "        M_f=None,\n",
    "        M_n=None,\n",
    "    ):\n",
    "        \n",
    "        # truncate because only first firing time matters\n",
    "        first_spks = self._first_spikes(outputs[-1])\n",
    "        max_f = np.max(first_spks)\n",
    "        \n",
    "        # TODO need to update weight.grad in torch framework\n",
    "        \n",
    "        all_fire=None\n",
    "        f_to_n={}\n",
    "        f_to_idx={}\n",
    "        for layer in range(len(outputs)):\n",
    "            for j, out in enumerate(outputs[layer]):\n",
    "                if all_fire is None:\n",
    "                    all_fire = out\n",
    "                else:\n",
    "                    all_fire = sortednp.merge(all_fire, out)\n",
    "                \n",
    "                for f in out:\n",
    "                    f_to_n[f] = (layer, j)\n",
    "           \n",
    "        F=0\n",
    "        all_fire = all_fire[all_fire <= max_f]\n",
    "        for f in all_fire:\n",
    "            f_to_idx[f] = F\n",
    "            F+=1\n",
    "        \n",
    "        W=0\n",
    "        # for converting 2D index in graph/layer\n",
    "        # to 1D index in IFT jacobian matrix\n",
    "        w_to_idx={}  \n",
    "        idx_to_w={}\n",
    "        \n",
    "        for layer in range(len(weights)):\n",
    "            for n, wts in enumerate(weights[layer]):\n",
    "                for j, w in enumerate(wts):\n",
    "                    w_to_idx[(layer, n, j)] = W\n",
    "                    idx_to_w[W] = (layer, n, j)\n",
    "                    W+=1\n",
    "        \n",
    "        # Initialize the IFT matrices\n",
    "        VF = sp.dok_matrix((F, F), dtype=np.float64)\n",
    "        VW = np.zeros((F, W))\n",
    "\n",
    "        for f in all_fire:\n",
    "            layer_idx, neuron_idx = f_to_n[f]\n",
    "            f_idx = f_to_idx[f]\n",
    "\n",
    "            # Retrieve causality diagram for this firing time\n",
    "            causal_in = M_f[f]\n",
    "            \n",
    "            # get all fire times for this neuron\n",
    "            fire_time = M_n[(layer_idx, neuron_idx)]\n",
    "            VF[f_idx, f_idx] += np.sum(self.theta*self.beta*np.exp(-self.beta*(f-fire_time))*np.heaviside(f-fire_time,0))\n",
    "\n",
    "            # Retrieve relevant info and calculate partial derivatives\n",
    "            for fin, n in causal_in:\n",
    "\n",
    "                # corresponding weight for this input firing time 'fin'\n",
    "                w = weights[layer_idx][neuron_idx][n]\n",
    "\n",
    "                # corresponding derivative w.r.t weight (accumulate)\n",
    "                v_ws = SNNUtil.V_w(f, fin, self.alpha, self.beta, self.theta)\n",
    "                w_idx = w_to_idx[(layer_idx, neuron_idx, n)]\n",
    "                VW[f_idx, w_idx] += v_ws\n",
    "\n",
    "                # for input layer, only need to derive w.r.t. its own firing time(s)\n",
    "                VF[f_idx, f_idx] += SNNUtil.V_fs(f, fin, w, self.alpha, self.beta, self.theta)\n",
    "\n",
    "                # for the rest, we also need to derive w.r.t. previous firing time(s)\n",
    "                if layer_idx != 0:\n",
    "                    v_fp = SNNUtil.V_fp(f, fin, w, self.alpha, self.beta, self.theta)\n",
    "                    VF[f_idx, f_to_idx[fin]] = v_fp\n",
    "            \n",
    "\n",
    "        try:\n",
    "            FW = sp.linalg.spsolve_triangular(VF.tocsr(), -VW, lower=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"VW=\", VW)\n",
    "            print(\"VF=\", VF)\n",
    "        \n",
    "        \n",
    "        # calculate CE part\n",
    "        final_out = outputs[-1]\n",
    "        first_spks = self._first_spikes(final_out)\n",
    "        \n",
    "        # store (accumulate) final grad\n",
    "        grad_out = [np.zeros_like(w, dtype=np.float64) for w in weights]\n",
    "        \n",
    "        for i in range(len(first_spks)):\n",
    "            if first_spks[i] >= self.T:\n",
    "                df_dW = np.zeros(W)\n",
    "                #print(\"zeros for df / dW at first spike for i=\", i)\n",
    "            else:\n",
    "                # find partial of spike time w.r.t. weights\n",
    "                f_idx = f_to_idx[first_spks[i]]\n",
    "                df_dW = FW[f_idx, :]  # pull out the row\n",
    "            \n",
    "            # calculate softmax at index for gradient calculation\n",
    "            p = softmax(-first_spks / self.tau_0)[i]\n",
    "            if i == label:\n",
    "                ce_partial = 1 / self.tau_0 * (1 - p)\n",
    "                ce_partial -= self.gamma / self.tau_1 * np.exp(first_spks[label] / self.tau_1)\n",
    "            else:\n",
    "                ce_partial = -1 / self.tau_0 * p\n",
    "            \n",
    "            for j, df_dw in enumerate(df_dW):\n",
    "                layer_idx, neuron_idx, w_idx = idx_to_w[j]\n",
    "                grad_out[layer_idx][neuron_idx][w_idx] += ce_partial * df_dw\n",
    "        \n",
    "        return grad_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "03db241d-3efe-43cd-b27f-2a7777fcefe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet = FeedForwardSNN(\n",
    "    in_dim=num_inputs,\n",
    "    beta=BETA, \n",
    "    alpha=ALPHA, \n",
    "    threshold=THETA, \n",
    "    layer_sizes=[num_hidden, num_classes],\n",
    "    weights=[fc1_weight,\n",
    "             fc2_weight]\n",
    ").build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "bdc56a74-58c7-4b4e-9f85-5eca7d172a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(\n",
    "    # (lr=0.001, decay1=0.9, decay2=0.999, eps=1e-07, clip_norm=None, lr_scheduler=None, **kwargs)\n",
    "    lr=ETA, \n",
    "    decay1=BETA_1, \n",
    "    decay2=BETA_2, \n",
    "    eps=EPS,\n",
    "    lr_scheduler=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "a2dfbc3c-aa9a-406c-bcc5-b66641c7742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = SnnCrossEntropy(\n",
    "    num_classes=num_classes,\n",
    "    T=max_t,\n",
    "    alpha=ALPHA,\n",
    "    beta=BETA,\n",
    "    theta=THETA,\n",
    "    tau_0=TAU_0,\n",
    "    tau_1=TAU_1,\n",
    "    gamma=GAMMA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "f5e7c946-29bc-480f-a4ce-10463fd30a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_example(data, labels, T, model, loss):\n",
    "    out = model.forward(data, T)\n",
    "    pred = np.argmin(np.asarray([np.min(out[1][-1][i], initial=T) for i in range(len(out[1][-1]))]))\n",
    "    acc = np.sum(pred == labels)\n",
    "    l = loss.forward(out[1][-1], labels)\n",
    "    grad = loss.backward(\n",
    "        out[0],\n",
    "        out[1],\n",
    "        out[2],\n",
    "        labels,\n",
    "        out[3],\n",
    "        out[4]\n",
    "    )\n",
    "    n_i = out[5]\n",
    "    return acc, l, grad, n_i\n",
    "    \n",
    "def batch_process_single(raw_data, raw_label):\n",
    "    data = raw_data.numpy() * (max_t-min_t) + min_t\n",
    "    idx = data.searchsorted(bias_t)\n",
    "    data = np.insert(data, idx, bias_t)\n",
    "    data = np.reshape(data, (num_inputs, 1))\n",
    "    label = raw_label.numpy()\n",
    "    a, l, g, ro = train_single_example(data,\n",
    "                                label,\n",
    "                                max_t,\n",
    "                                nnet,\n",
    "                                ce_loss)\n",
    "    return a, l, g, ro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "21ea1420-2e16-4803-9c03-c2031075c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training parameters\n",
    "n_epochs = 5\n",
    "ntrain_accuracies = []\n",
    "ntrain_losses = []\n",
    "\n",
    "train_data = YinYangDataset(size=5000, seed=42)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batchsize_train, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc3f066-f9bc-4789-be47-75b7fb17aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "with Parallel(n_jobs=-1) as parallel:\n",
    "    for k in range(n_epochs):\n",
    "        for i, (batch, batch_labels) in enumerate(train_loader):\n",
    "            \n",
    "            bsz = len(batch)\n",
    "\n",
    "            a, l, g, batch_ro = zip(*parallel(\n",
    "                delayed(batch_process_single)\n",
    "                (batch[j], batch_labels[j])\n",
    "                for j in range(bsz)\n",
    "            ))\n",
    "\n",
    "            batch_acc = np.mean(a)\n",
    "            batch_loss = np.mean(l)\n",
    "\n",
    "            batch_grad = [np.zeros_like(w, dtype=np.float64) for w in nnet.weights]\n",
    "\n",
    "            for grad in g:\n",
    "                batch_grad = [batch_grad[k] + 1/bsz * grad[k] for k in range(len(grad))]\n",
    "\n",
    "            ntrain_accuracies.append(batch_acc)\n",
    "            ntrain_losses.append(batch_loss)\n",
    "            new_weights = [adam.update(nnet.weights[i], \n",
    "                                       batch_grad[i], \n",
    "                                       \"w_layer_{i}\".format(i=i)\n",
    "                                      ) \n",
    "                           for i in range(len(batch_grad))]\n",
    "            #new_weights = [net.weights[i] - ETA * batch_grad[i] for i in range(len(batch_grad))]\n",
    "            nnet.update(new_weights, max_t, batch_ro, -10)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                end_time = time.perf_counter()\n",
    "                print(\"Time elapsed (sec)=\", end_time - start_time)\n",
    "                print(\"epoch=\", k)\n",
    "                print(\"loss=\", batch_loss)\n",
    "                print(\"acc=\", batch_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9190de90-1292-439e-bfe9-979f5998d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ntrain_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f9d3f3-c24b-43fd-b627-0b50ea0864f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ntrain_losses, color='C0')\n",
    "plt.plot(train_losses.squeeze().detach().numpy(), color='C1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf97754-c12e-4d96-84dd-cda64b4b38f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-nb-default",
   "language": "python",
   "name": "jupyter-nb-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
